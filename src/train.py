import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
from tqdm import tqdm

# Importujemy nasze moduÅ‚y
from ai_model import SimpleUNet
from dataset import ImageDataset

# --- KONFIGURACJA TRENINGU ---
# MoÅ¼esz eksperymentowaÄ‡ z tymi liczbami w pracy
EPOCHS = 100           # Ile razy przerobimy caÅ‚y zbiÃ³r (na start 20 jest ok)
BATCH_SIZE = 8        # Ile zdjÄ™Ä‡ naraz (zmniejsz do 2, jeÅ›li wywali bÅ‚Ä…d pamiÄ™ci)
LEARNING_RATE = 0.001 # SzybkoÅ›Ä‡ uczenia (standardowa wartoÅ›Ä‡)
IMG_SIZE = 256        # Rozmiar obrazkÃ³w do treningu
# -----------------------------

def train_task(task_name):
    print(f"\n" + "="*40)
    print(f" ðŸš€ START TRENINGU: {task_name.upper()}")
    print("="*40)
    
    # 1. WybÃ³r urzÄ…dzenia (GPU nvidia lub procesor)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"UrzÄ…dzenie obliczeniowe: {device}")

    # 2. Dane
    dataset = ImageDataset(task_type=task_name, img_size=IMG_SIZE)
    if len(dataset) == 0:
        return # Przerywamy, jeÅ›li brak danych
        
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    print(f"Liczba zdjÄ™Ä‡: {len(dataset)}")
    print(f"Liczba krokÃ³w na epokÄ™: {len(dataloader)}")

    # 3. Model
    model = SimpleUNet().to(device)
    
    # 4. NarzÄ™dzia uczenia
    criterion = nn.L1Loss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # 5. GÅ‚Ã³wna pÄ™tla
    model.train()
    
    for epoch in range(EPOCHS):
        epoch_loss = 0.0
        # Pasek postÄ™pu
        progress = tqdm(dataloader, desc=f"Epoka {epoch+1}/{EPOCHS}", unit="batch")
        
        for inputs, targets in progress:
            inputs, targets = inputs.to(device), targets.to(device)

            # Zerowanie starej wiedzy o bÅ‚Ä™dach
            optimizer.zero_grad()

            # A. SieÄ‡ prÃ³buje zgadnÄ…Ä‡ (Forward)
            outputs = model(inputs)

            # B. Liczymy jak bardzo siÄ™ pomyliÅ‚a (Loss)
            loss = criterion(outputs, targets)

            # C. Nauka (Backward)
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            progress.set_postfix(loss=loss.item())

        avg_loss = epoch_loss / len(dataloader)
        # print(f"Åšredni bÅ‚Ä…d w epoce {epoch+1}: {avg_loss:.6f}")

    # 6. Zapisywanie wytrenowanego modelu
    save_dir = Path('models')
    save_dir.mkdir(exist_ok=True)
    
    model_path = save_dir / f"model_{task_name}.pth"
    torch.save(model.state_dict(), model_path)
    
    print("\n" + "="*40)
    print(f"âœ… TRENING ZAKOÅƒCZONY!")
    print(f"Model zapisano w: {model_path}")
    print("="*40 + "\n")

if __name__ == "__main__":
    # Tutaj decydujesz, co trenujesz. 
    # Na razie uruchomimy tylko SZUM (noise).
    
    train_task("noise")
    
    # PÃ³Åºniej odkomentujesz te linie:
    # train_task("blur")
    # train_task("low_res")